Q()
q()
q()
tall.packages("stargazer") # para sacar tablas de resultados de modelos de regresión
install.packages("mfx") # para calcular efectos marginales en modelos de
install.packages("scorecard") # funciones para hacer tarjetas de puntuación
install.packages("gmodels") # algunas funciones como CrossTable() para analizar relaciones bivariantes
install.packages("dplyr")  # Filtrar y seleccionar variables
install.packages("ggplot2") # Para realizar gráficos
install.packages("tseries") # Adf.test, kpss.test, bds.test, get.hist.quote, portfolio.optim, surrogate, arma, garch
install.packages("forecast") # interpolar, ndiffs para saber el número de diferencias, adf, ggCcf(x,y): [xt+k, yt], ggAcf, autoarima
install.packages ("zoo") #diferentes procesos para series temporales
install.packages("stargazer") # para tablas de resultados
install.packages("tempdisagg") #ta agrega (reduce frecuencia), td desagrega (aumenta frecuencias)
install.packages("excel.link") #para cargar archivos excel
install.packages("pxR") # para leer datos desde archivos .px pc-axis del INE pxR::read.px()
install.packages("eurostat") #para leer datos de eurostat
install.packages("WDI") # para leer datos del Banco Mundial
install.packages("data.table")
install.packages("xts") #para series temporales grandes
install.packages("dygraphs")
install.packages("outliers") #outlier, rm.outlier, scores, chisq.out.test # para detectar outliers o datos anómalos ojo serie estacionaria
install.packages("seasonal") #Para extraer cmponente estacional
install.packages("tempdisagg") #ta agrega (reduce frecuencia), td desagrega (aumenta frecuencias)
install.packages("urca")# Para test de raíces Unitarias
install.packages("ecm") #para modelos de correción de error
install.packages("lmtest") #varios test de modelos de regresión
install.packages("sandwich") #estimación robusta matriz de varianzas y covarianzas
install.packages("rugarch") #para la estimación de modelos Garch
install.packages("zoo") #diferentes procesos para series temporales
options(warn=-1)
knitr::opts_chunk$set(echo = TRUE)
library("stargazer") # para sacar tablas de resultados de modelos de regresión
library("mfx") # para calcular efectos marginales en modelos de
library("scorecard") # funciones para hacer tarjetas de puntuación
library("gmodels") # algunas funciones como CrossTable() para analizar relaciones bivariantes
library("dplyr")  # Filtrar y seleccionar variables
library("ggplot2") # Para realizar gráficos
library("tseries") # Adf.test, kpss.test, bds.test, get.hist.quote, portfolio.optim, surrogate, arma, garch
install.packages("tseries") # Adf.test, kpss.test, bds.test, get.hist.quote, portfolio.optim, surrogate, arma, garch
library("tseries") # Adf.test, kpss.test, bds.test, get.hist.quote, portfolio.optim, surrogate, arma, garch
library("quadprog") # Para poder instalar tseries
install.packages("quadprog")
install.packages("forecast") # interpolar, ndiffs para saber el número de diferencias, adf, ggCcf(x,y): [xt+k, yt], ggAcf, autoarima
install.packages("zoo") #diferentes procesos para series temporales
install.packages("zoo")
options(warn=-1)
knitr::opts_chunk$set(echo = TRUE)
library("forecast") # interpolar, ndiffs para saber el número de diferencias, adf, ggCcf(x,y): [xt+k, yt], ggAcf, autoarima
install.packages("forecast") # interpolar, ndiffs para saber el número de diferencias, adf, ggCcf(x,y): [xt+k, yt], ggAcf, autoarima
install.packages("zoo") #diferentes procesos para series temporales
install.packages("stargazer") # para tablas de resultados
install.packages("tempdisagg") #ta agrega (reduce frecuencia), td desagrega (aumenta frecuencias)
install.packages("excel.link") #para cargar archivos excel
install.packages("pxR") # para leer datos desde archivos .px pc-axis del INE pxR::read.px()
install.packages("eurostat") #para leer datos de eurostat
install.packages("WDI") # para leer datos del Banco Mundial
install.packages("data.table")
install.packages("xts") #para series temporales grandes
install.packages("data.table")
install.packages("data.table")
install.packages("data.table")
install.packages("data.table")
options(warn=-1)
knitr::opts_chunk$set(echo = TRUE)
library("forecast") # interpolar, ndiffs para saber el número de diferencias, adf, ggCcf(x,y): [xt+k, yt], ggAcf, autoarima
install.packages("forecast") # interpolar, ndiffs para saber el número de diferencias, adf, ggCcf(x,y): [xt+k, yt], ggAcf, autoarima
knitr::include_graphics("PIB.png")
knitr::include_graphics("PIB.png")
knitr::include_graphics("PIB.png")
yy.ts<-(y.ts/stats::lag(y.ts,k = -4)-1)
?stats::lag
stats::lag(c(1,2,3),2)
stats::lag(c(1,2,3,4,5,6,7,8,9,10),2)
stats::lag(c(1,2,3,4,5,6,7,8,9,10),-2)
stats::lag(c(1,2,3,4,5,6,7,8,9,10),-4)
stats::lag(c(1,2,3,4,5,6,7,8,9,10),-8)
stats::lag(c(1,2,3,4,5,6,7,8,9,10),-10)
stats::lag(c(1,2,3,4,5,6,7,8,9,10),-2)
stats::lag(c(1,2,3,4,5,6,7,8,9,10),-1)
stats::lag(c(1,2,3,4,5,6,7,8,9,10),1)
stats::lag(c(1,2,3,4,5,6,7,8,9,12),1)
stats::lag(c(1,2,3,4,5,6,7,8,9,12),-1)
stats::lag(c(1,2,3,4,5,6,7,8,9,12),-2)
seq(1,10)
seq(1,10) / stats::lag(seq(1,10),-4)
?grangertest
library(lmtest)
?grangertest
?train
??train
# load germancredit data
load("datriesgos")
q
q()
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
aux<- as.data.frame(readRDS("./tab/weirdPass"))
setwd("~/Desktop/Santi/master_BD/machine_learning/entrega/")
aux<- as.data.frame(readRDS("./tab/weirdPass"))
###########################################################
# Competición Titanic Kaggle
# Algoritmo tipo: ENSAMBLADO
# Resultado-mean: 0.78497
# Resultado-mean_pond: 0.79425
# Resultado-best:
# Resultado-K-means:
# Posición: No mejora
# Percentil: No mejora
# Fecha: 15 - 05 - 2019
###########################################################
setwd("~/Desktop/Santi/master_BD/machine_learning/entrega/")
setwd("/cloud/project/entrega/")
setwd("C:\\Users/n230104/Desktop/personal/master/entrega/entrega/")
source("funcionesML.R")
# 0. Cargo paquetes ----
library(dplyr)
library(reshape2)
library(ggplot2)
library(data.table)
library(stringr)
library(tidyr)
library(gridExtra)
library(questionr)
library(caret)
library(cluster)
library(ggplot2)
library(factoextra)
library(FactoMineR)
library(NbClust)
# 1. Carga de datos ----
datTrain<- readRDS("train_clean")
datTest<- readRDS("test_clean")
datTotal<- rbind(datTrain[,c(2:ncol(datTrain))], datTest)
datTrain_sca<- readRDS("train_clean_sca")
datTest_sca<- readRDS("test_clean_sca")
datTotal_sca<- rbind(datTrain_sca[,c(2:ncol(datTrain_sca))], datTest_sca)
## 1.1. Carga de predicciones ----
probTrain<- readRDS("./tab/probTrain.csv")
probTest<- readRDS("./tab/probTest.csv")
probTotal<- rbind(probTrain[,c(1,3:ncol(probTrain))], probTest)
binTrain<- readRDS("./tab/binTrain.csv")
binTest<- readRDS("./tab/binbTest.csv")
binTotal<- rbind(binTrain[,c(1,3:ncol(binTrain))], binTest)
# Submission
submission<- data.frame(PassengerId = as.numeric(row.names(datTest_sca)))
## 1.2. Ponderación
ranking<- readRDS("./tab/ranking")
total<- readRDS("./tab/cv_all")
metricMean<- aggregate(metric~modelo, data=total, mean)
# Fix ponderation values between 0.3 - 0.7
metricMean$pond<- ((metricMean$metric - min(metricMean$metric))*0.4/(max(metricMean$metric)-min(metricMean$metric)))+0.3
metricMean$pond<- metricMean$pond/sum(metricMean$pond)
#% Sort values as they are in names(ProbTotal)
posi<- c()
for(mod in metricMean$modelo){
posi<- c(posi, which(names(probTotal[,c(2:ncol(probTotal))]) == mod))
}
metricMean$posi<- posi
metricMean[order(metricMean$posi),]
# 2. Average predictions ----
# Aggregate the mean and pondered mean probability for each sample
probTotal$mean<- rowMeans(probTotal[,c(2:ncol(probTotal))])
probTotal$mean_pond<- rowSums(metricMean$pond * probTotal[,c(2:(ncol(probTotal)-1))])
# Reescaling
probTotal$mean_pond<- (probTotal$mean_pond - min(probTotal$mean_pond))/(max(probTotal$mean_pond) - min(probTotal$mean_pond))
binTotal_man<- binTotal
for(col in names(binTotal[,c(2:ncol(binTotal))])){
binTotal_man[[col]]<- as.numeric(ifelse(binTotal_man[[col]]==0,0,1))
}
binTotal$mean<- as.factor(ifelse(rowMeans(binTotal_man[,c(2:ncol(binTotal_man))]) < 0.5,0,1))
binTotal$mean_pond<- as.factor(ifelse(rowSums(metricMean$pond * binTotal_man[,c(2:ncol(binTotal_man))]) < 0.5,0,1))
## 2.1 Doubts ----
# Calculate how many samples will switch if we use a different way to evaluate it
doubts<- c()
for(i in seq(1,nrow(binTotal))){
a<- all(as.numeric(binTotal[i,c(2:ncol(binTotal))]) == as.numeric(binTotal[i,2]))
doubts<- c(doubts, ifelse(a, 0, 1))
}
sum(doubts[1:nrow(probTrain)])/nrow(probTrain)
sum(doubts[(nrow(probTrain)+1):nrow(probTotal)])/nrow(probTest)
## 2.2 Mean submissions ----
Sub_mean<- submission
Sub_mean$Survived<- binTotal$mean[(nrow(probTrain)+1):nrow(probTotal)]
fwrite(Sub_mean, "./sub/mean_sub.csv", row.names = FALSE)
Sub_mean_pond<- submission
Sub_mean_pond$Survived<- binTotal$mean_pond[(nrow(probTrain)+1):nrow(probTotal)]
fwrite(Sub_mean_pond, "./sub/mean-pond_sub.csv", row.names = FALSE)
# 3. Heatmap ----
hm_df<- data.frame(PassengerID = row.names(datTrain))
for (col in names(binTotal[,2:ncol(binTotal)])){
binTotal_sel<- as.numeric(binTotal[c(1:nrow(datTrain)), c(col)])
hm_df[[col]]<- as.factor(ifelse(binTotal_sel == as.numeric(datTrain$Survived), "HIT", "FAIL"))
}
hm<- melt(hm_df, measure.vars = names(hm_df[,c(2:ncol(hm_df))]), factorsAsStrings = FALSE)
ggplot(data = hm, aes(x = PassengerID, y = variable, fill= value)) +
geom_tile() +
ggtitle("Models over Train set") +
scale_fill_manual(values=c("#ce3939", "#86cc7a"), breaks=levels(hm$value)) +
theme_update(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
axis.text.x = element_blank(),
axis.title.y = element_blank(),
axis.ticks.x = element_blank())
## 3.1 Weird Cases ----
weird_ids<- as.numeric(hm_df[which(hm_df$bagg_full == "FAIL"),]$PassengerID)
weird_pass<- datTrain[which(rownames(datTrain) %in% weird_ids),]
weird_pass
saveRDS(weird_pass, "./tab/weirdPass")
aux<- as.data.frame(readRDS("./tab/weirdPass"))
kable(aux, row.names= FALSE)
colnames(aux<- c("Sur.", "Cl", "Sex", "Age", "SSp", "Pch", "Fare", "Emb", "CabL", "CabB",
"TickN", "Title", "SnFr", "FS", "TickLG", "ran"))
colnames(aux)<- c("Sur.", "Cl", "Sex", "Age", "SSp", "Pch", "Fare", "Emb", "CabL", "CabB",
"TickN", "Title", "SnFr", "FS", "TickLG", "ran"))
aux<- as.data.frame(readRDS("./tab/weirdPass"))
colnames(aux)<- c("Sur.", "Cl", "Sex", "Age", "SSp", "Pch", "Fare", "Emb", "CabL", "CabB",
"TickN", "Title", "SnFr", "FS", "TickLG", "ran"))
colnames(aux)<- c("Sur.", "Cl", "Sex", "Age", "SSp", "Pch", "Fare", "Emb", "CabL", "CabB",
"TickN", "Title", "SnFr", "FS", "TickLG", "ran")
kable(aux, row.names= FALSE)
aux<- as.data.frame(readRDS("./tab/weirdPass"))[,c(1:13)]
colnames(aux)<- c("Sur.", "Cl", "Sex", "Age", "SSp", "Pch", "Fare", "Emb", "CabL", "CabB",
"TickN", "Title", "SnFr")
kable(aux, row.names= FALSE)
# 4. Hierarchy clustering ----
# Remove factors
num_cols<- names(which(sapply(datTotal_sca, is.numeric) == TRUE))
fac_cols<- names(datTotal_sca)[!(names(datTotal_sca) %in% num_cols)]
# Join all the measurable data
fullSet<- cbind(datTotal_sca[,num_cols], probTotal[, c(2:ncol(probTotal))])
summary(datTotal_sca)
# Distancias
distances <- dist(fullSet, method = "euclidean")
fviz_dist(distances, show_labels = FALSE)
library(dplyr)
library(reshape2)
library(ggplot2)
library(data.table)
library(stringr)
library(tidyr)
library(gridExtra)
library(questionr)
library(caret)
library(cluster)
library(ggplot2)
library(factoextra)
library(FactoMineR)
library(NbClust)
install.packages(c("betareg", "car", "dbplyr", "digest", "dplyr", "e1071", "evaluate", "gower", "knitr", "labelled", "markdown", "numDeriv", "onewaytests", "openxlsx", "pillar", "progress", "quantreg", "RcppArmadillo", "reprex", "RJSONIO", "rmarkdown", "tibble", "tinytex", "tseries", "xfun", "zip", "zoo"))
install.packages("factoextra")
install.packages("FactoMiner")
install.packages("FactoMineR")
install.packages("NbClust")
install.packages("cluster")
install.packages("reshape2")
library(dplyr)
library(reshape2)
library(ggplot2)
library(data.table)
library(stringr)
library(tidyr)
library(gridExtra)
library(questionr)
library(caret)
library(cluster)
library(ggplot2)
library(factoextra)
library(FactoMineR)
library(NbClust)
# 4. Hierarchy clustering ----
# Remove factors
num_cols<- names(which(sapply(datTotal_sca, is.numeric) == TRUE))
fac_cols<- names(datTotal_sca)[!(names(datTotal_sca) %in% num_cols)]
# Join all the measurable data
fullSet<- cbind(datTotal_sca[,num_cols], probTotal[, c(2:ncol(probTotal))])
summary(datTotal_sca)
# Distancias
distances <- dist(fullSet, method = "euclidean")
fviz_dist(distances, show_labels = FALSE)
res.hc <- hclust(distances, method="ward.D2")
fviz_dend(res.hc, k = 10, # Cut in four groups
cex = 0.5, # label size
k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07", "#cc71d6", "#8c0000",
"#9b5726", "#a30488", "#016384", "#a168e2"),
color_labels_by_k = TRUE, # color labels by groups
rect = TRUE, # Add rectangle around groups
main = "Clúster Jerárquico"
)
# Distancias
distances <- dist(fullSet, method = "euclidean")
fviz_dist(distances, show_labels = FALSE)
res.hc <- hclust(distances, method="ward.D2")
grp <- cutree(res.hc, k = 10)
table(grp)
fviz_cluster(list(data = fullSet, cluster = grp),
# palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
ellipse.type = "convex", # Concentration ellipse
geom = c("point"),
repel = TRUE, # Avoid label overplotting (slow)
show.clust.cent = TRUE, ggtheme = theme_minimal(),
)
